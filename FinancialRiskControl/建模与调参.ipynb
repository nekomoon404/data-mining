{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.知识补充\n",
    "\n",
    "* 数据集的划分有三种方式：留出法，交叉验证法和自助法（常使用的是k折交叉验证，另外两种不常用？）\n",
    "  1. 对于数据量充足的时候，通常采用留出法或者k折交叉验证来进行训练/测试集的部分\n",
    "  2. 对于数据集小且难以有效划分训练/测试集时使用自助法\n",
    "  3. 对于数据集小且可有效划分的时候最好使用留一法来进行划分，因为这种方法最为准确\n",
    "  \n",
    "* 学习了XGBoost的原理，以及在小数据集上用xgb.cv进行调参，比赛的数据有些多，只用baseline中的参数跑了一下。\n",
    "  >记录了下XGBoost原理的学习笔记：[XGBoost原理简述](https://nekomoon404.github.io/2020/09/22/XGBoost%E5%8E%9F%E7%90%86%E7%AE%80%E8%BF%B0/)\n",
    "\n",
    "* 学习了LightGBM如何在XGBoost的基础上做出改进，以及用网格搜索的方法逐步对lgb调参。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.建模"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostRegressor\n",
    "import warnings\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, log_loss\n",
    "warnings.filterwarnings('ignore')\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reduce_mem_usage 函数通过调整数据类型，帮助我们减少数据在内存中占用的空间\n",
    "def reduce_mem_usage(df):\n",
    "    start_mem = df.memory_usage().sum() \n",
    "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        \n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "        else:\n",
    "            df[col] = df[col].astype('category')\n",
    "\n",
    "    end_mem = df.memory_usage().sum() \n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 716800128.00 MB\n",
      "Memory usage after optimization is: 128000128.00 MB\n",
      "Decreased by 82.1%\n",
      "Memory usage of dataframe is 177600128.00 MB\n",
      "Memory usage after optimization is: 31800128.00 MB\n",
      "Decreased by 82.1%\n"
     ]
    }
   ],
   "source": [
    "#读取特征工程处理好的数据\n",
    "data_train = pd.read_csv('data_train_for_model-09-21.csv')\n",
    "data_train = reduce_mem_usage(data_train)\n",
    "\n",
    "data_test = pd.read_csv('data_test_for_model-09-21.csv')\n",
    "data_test = reduce_mem_usage(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#训练数据/测试数据准备\n",
    "features = [f for f in data_train.columns if f not in ['id','isDefault']]\n",
    "\n",
    "x_train = data_train[features]\n",
    "x_test = data_test[features]\n",
    "\n",
    "y_train = data_train['isDefault']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#处理样本不均衡\n",
    "#from imblearn.combine import SMOTETomek    \n",
    "#smote_tomek = SMOTETomek(random_state=0)\n",
    "#x_train, y_train = smote_tomek.fit_resample(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5折交叉验证\n",
    "folds = 5\n",
    "seed = 0\n",
    "kf = KFold(n_splits=folds, shuffle=True, random_state=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用Lightgbm进行建模\n",
    "\n",
    ">Lightgbm调参的方法可以参考：[LightGBM调参方法（具体操作）](https://www.cnblogs.com/bjwu/p/9307344.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#使用Lightgbm进行建模，这里使用了baseline中提供的参数，从训练结果看模型是存在的过拟合的，说明还有调参空间\n",
    "#模型训练  lgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.先用lightgbm库原生的.cv，确定下迭代的次数num_boost_round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\tcv_agg's auc: 0.727105 + 0.0014403\n",
      "[100]\tcv_agg's auc: 0.730844 + 0.00148981\n",
      "[150]\tcv_agg's auc: 0.73261 + 0.00145055\n",
      "[200]\tcv_agg's auc: 0.733577 + 0.00136686\n",
      "[250]\tcv_agg's auc: 0.734204 + 0.00134081\n",
      "[300]\tcv_agg's auc: 0.734521 + 0.00128866\n",
      "[350]\tcv_agg's auc: 0.734812 + 0.00130706\n",
      "[400]\tcv_agg's auc: 0.73493 + 0.00136635\n",
      "[450]\tcv_agg's auc: 0.73491 + 0.00139679\n",
      "[500]\tcv_agg's auc: 0.734941 + 0.00145647\n",
      "[550]\tcv_agg's auc: 0.734876 + 0.001415\n",
      "01:36:888110\n",
      "best n_estimators: 490\n",
      "best cv score: 0.7349676565422735\n"
     ]
    }
   ],
   "source": [
    "params = {'objective' : 'binary',\n",
    "         #is_unbalance = True,\n",
    "         'metric' : 'auc',\n",
    "         'max_depth' : 6,\n",
    "         'num_leaves' : 40,\n",
    "         'learning_rate' : 0.1,\n",
    "         'feature_fraction' : 0.8,\n",
    "         'bagging_fraction' : 0.8,\n",
    "         'min_child_samples' : 21,     #和min_data_in_leaf 同义，设置的较大可以避免生成一个过深的树，但有可能过拟合\n",
    "         'min_child_weight': 0.001,    #和min_sum_hessian_in_leaf同义， 使一个节点分裂的最小海森值之和？不懂\n",
    "         \n",
    "         'bagging_freq': 2,   #降采样频率，表示bagging的频率，k意味着每k轮迭代进行一次bagging？\n",
    "         'reg_alpha': 0,      #L1正则化参数\n",
    "         'reg_lambda' : 0      #L2正则化参数\n",
    "         }\n",
    "\n",
    "lgb_data_train = lgb.Dataset(x_train, y_train, silent=True)\n",
    "\n",
    "time0 = time()\n",
    "cv_results = lgb.cv(\n",
    "    params, lgb_data_train, num_boost_round=1200, nfold=5, stratified=False, shuffle=True, metrics='auc',\n",
    "    early_stopping_rounds=100, verbose_eval=50, show_stdv=True, seed=0)\n",
    "print(datetime.datetime.fromtimestamp(time()-time0).strftime(\"%M:%S:%f\"))\n",
    "\n",
    "print('best n_estimators:', len(cv_results['auc-mean']))\n",
    "print('best cv score:', cv_results['auc-mean'][-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "用lgb.cv确定的迭代次数是490。\n",
    "\n",
    "接下来用网格搜索来确定调其他参数，网格搜索要使用sklearn的GridSearchCV，先调对模型影响大的参数，顺序可以是：\n",
    "\n",
    "注意每一步得到优化参数后要在下一步手动更新，网格搜索是真的费时间，有哪些调参的trick？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"通过网格搜索确定最优参数\"\"\"\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def get_best_cv_params(learning_rate=0.1, n_estimators=490, num_leaves=40, max_depth=6, bagging_fraction=0.8, \n",
    "                       feature_fraction=0.8, bagging_freq=2, min_child_samples=21, min_child_weight=0.001, \n",
    "                       min_split_gain=0, reg_lambda=0, reg_alpha=0, param_grid=None):\n",
    "    # 设置5折交叉验证\n",
    "    cv_fold = StratifiedKFold(n_splits=5, random_state=0, shuffle=True, )\n",
    "    \n",
    "    model_lgb = lgb.LGBMClassifier(learning_rate=learning_rate,\n",
    "                                   n_estimators=n_estimators,\n",
    "                                   \n",
    "                                   num_leaves=num_leaves,\n",
    "                                   max_depth=max_depth,\n",
    "                                   \n",
    "                                   bagging_fraction=bagging_fraction,\n",
    "                                   feature_fraction=feature_fraction,\n",
    "                                   bagging_freq=bagging_freq,\n",
    "                                   \n",
    "                                   min_child_samples=min_child_samples,\n",
    "                                   min_child_weight=min_child_weight,\n",
    "                                   \n",
    "                                   reg_lambda=reg_lambda,\n",
    "                                   reg_alpha=reg_alpha,\n",
    "                                   \n",
    "                                   min_split_gain=min_split_gain,\n",
    "                                   \n",
    "                                   n_jobs= -1,\n",
    "                                  )\n",
    "    grid_search = GridSearchCV(estimator=model_lgb, \n",
    "                               cv=cv_fold,\n",
    "                               param_grid=param_grid,\n",
    "                               scoring='roc_auc'\n",
    "                              )\n",
    "    grid_search.fit(x_train, y_train)\n",
    "\n",
    "    print('模型当前最优参数为:{}'.format(grid_search.best_params_))\n",
    "    print('模型当前最优得分为:{}'.format(grid_search.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.调max_depth和num_leaves，策略是先粗调后细调\n",
    "\n",
    "* max_depth：设置树深度，深度越大可能过拟合\n",
    "* num_leaves：因为 LightGBM 使用的是 leaf-wise 的算法，因此在调节树的复杂程度时，使用的是 num_leaves 而不是 max_depth。大致换算关系：num_leaves = 2^(max_depth)，但是它的值的设置应该小于 2^(max_depth)，否则可能会导致过拟合。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型当前最优参数为:{'max_depth': 5, 'num_leaves': 35}\n",
      "模型当前最优得分为:0.7354909075787674\n",
      "time:  17:32:796533\n"
     ]
    }
   ],
   "source": [
    "lgb_params = {'num_leaves': range(10, 80, 5), 'max_depth': range(3,10,2)}\n",
    "\n",
    "time0 = time()\n",
    "get_best_cv_params(learning_rate=0.1, n_estimators=490, num_leaves=None, max_depth=None, bagging_fraction=0.8, \n",
    "                       feature_fraction=0.8, bagging_freq=2, min_child_samples=21, min_child_weight=0.001, \n",
    "                       min_split_gain=0, reg_lambda=0, reg_alpha=0, param_grid=lgb_params)     #要调的参数设置成None？\n",
    "print('time: ', datetime.datetime.fromtimestamp(time()-time0).strftime(\"%M:%S:%f\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "进一步细调max_depth，和num_leaves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型当前最优参数为:{'max_depth': 5, 'num_leaves': 31}\n",
      "模型当前最优得分为:0.7355489458951782\n",
      "time:  52:46:757414\n"
     ]
    }
   ],
   "source": [
    "lgb_params = {'num_leaves': range(30, 40, 1), 'max_depth': range(3,7,1)}\n",
    "\n",
    "time0 = time()\n",
    "get_best_cv_params(learning_rate=0.1, n_estimators=490, num_leaves=None, max_depth=None, bagging_fraction=0.8, \n",
    "                       feature_fraction=0.8, bagging_freq=2, min_child_samples=21, min_child_weight=0.001, \n",
    "                       min_split_gain=0, reg_lambda=0, reg_alpha=0, param_grid=lgb_params)     #要调的参数设置成None？\n",
    "print('time: ', datetime.datetime.fromtimestamp(time()-time0).strftime(\"%M:%S:%f\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.调min_child_samples 和 min_child_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型当前最优参数为:{'min_child_samples': 23, 'min_child_weight': 0.001}\n",
      "模型当前最优得分为:0.7356996328240119\n",
      "time:  16:59:609068\n"
     ]
    }
   ],
   "source": [
    "lgb_params={\n",
    "    'min_child_samples': [18, 19, 20, 21, 22, 23],    #看别人这样设置，不太懂\n",
    "    'min_child_weight':[0.001, 0.002]\n",
    "}\n",
    "\n",
    "time0 = time()\n",
    "get_best_cv_params(learning_rate=0.1, n_estimators=490, num_leaves=31, max_depth=5, bagging_fraction=0.8, \n",
    "                       feature_fraction=0.8, bagging_freq=2, min_child_samples=None, min_child_weight=None, \n",
    "                       min_split_gain=0, reg_lambda=0, reg_alpha=0, param_grid=lgb_params)     #要调的参数设置成None？\n",
    "print('time: ', datetime.datetime.fromtimestamp(time()-time0).strftime(\"%M:%S:%f\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.调feature_fraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型当前最优参数为:{'feature_fraction': 0.8}\n",
      "模型当前最优得分为:0.7356996328240119\n",
      "time:  06:54:594004\n"
     ]
    }
   ],
   "source": [
    "lgb_params = {#'bagging_fraction': [i/10 for i in range(5,10,1)], \n",
    "              'feature_fraction': [i/10 for i in range(5,10,1)],\n",
    "              #'bagging_freq': range(0,81,10)\n",
    "             }\n",
    "\n",
    "time0 = time()\n",
    "get_best_cv_params(learning_rate=0.1, n_estimators=490, num_leaves=31, max_depth=5, bagging_fraction=0.8, \n",
    "                       feature_fraction=None, bagging_freq=2, min_child_samples=23, min_child_weight=0.001, \n",
    "                       min_split_gain=0, reg_lambda=0, reg_alpha=0, param_grid=lgb_params)     #要调的参数设置成None？\n",
    "print('time: ', datetime.datetime.fromtimestamp(time()-time0).strftime(\"%M:%S:%f\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.调bagging_fraction 和 bagging_freq\n",
    "\n",
    "这两个参数必须同时设置，bagging_fraction相当于subsample样本采样，可以使bagging更快的运行，同时也可以降拟合。bagging_freq默认0，表示bagging的频率，0意味着没有使用bagging，k意味着每k轮迭代进行一次bagging。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型当前最优参数为:{'bagging_fraction': 0.8, 'bagging_freq': 2}\n",
      "模型当前最优得分为:0.7356996328240119\n",
      "time:  24:11:655037\n"
     ]
    }
   ],
   "source": [
    "lgb_params = {'bagging_fraction': [i/10 for i in range(5,10,1)], \n",
    "              #'feature_fraction': [i/10 for i in range(5,10,1)],\n",
    "              'bagging_freq': range(2,6,1)    #这个范围不太确定\n",
    "             }\n",
    "\n",
    "time0 = time()\n",
    "get_best_cv_params(learning_rate=0.1, n_estimators=490, num_leaves=31, max_depth=5, bagging_fraction=None, \n",
    "                       feature_fraction=0.8, bagging_freq=None, min_child_samples=23, min_child_weight=0.001, \n",
    "                       min_split_gain=0, reg_lambda=0, reg_alpha=0, param_grid=lgb_params)     #要调的参数设置成None？\n",
    "print('time: ', datetime.datetime.fromtimestamp(time()-time0).strftime(\"%M:%S:%f\"))\n",
    "#这两轮搜了个寂寞"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.调L1正则化系数reg_alpha，L2正则化系数reg_lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型当前最优参数为:{'reg_alpha': 0.5, 'reg_lambda': 0.3}\n",
      "模型当前最优得分为:0.7357520426784678\n",
      "time:  07:44:422930\n"
     ]
    }
   ],
   "source": [
    "lgb_params = {\n",
    "    'reg_alpha': [0, 0.001, 0.01, 0.03, 0.08, 0.3, 0.5],\n",
    "    'reg_lambda': [0, 0.001, 0.01, 0.03, 0.08, 0.3, 0.5]\n",
    "}\n",
    "time0 = time()\n",
    "get_best_cv_params(learning_rate=0.1, n_estimators=490, num_leaves=31, max_depth=5, bagging_fraction=0.8, \n",
    "                       feature_fraction=0.8, bagging_freq=2, min_child_samples=23, min_child_weight=0.001, \n",
    "                       min_split_gain=0, reg_lambda=None, reg_alpha=None, param_grid=lgb_params)     #要调的参数设置成None？\n",
    "print('time: ', datetime.datetime.fromtimestamp(time()-time0).strftime(\"%M:%S:%f\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.调min_split_gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型当前最优参数为:{'min_split_gain': 0.0}\n",
      "模型当前最优得分为:0.7357520426784678\n",
      "time:  18:51:284698\n"
     ]
    }
   ],
   "source": [
    "lgb_params = {'min_split_gain': [i/10 for i in range(0,11,1)]}\n",
    "\n",
    "time0 = time()\n",
    "get_best_cv_params(learning_rate=0.1, n_estimators=490, num_leaves=31, max_depth=5, bagging_fraction=0.8, \n",
    "                       feature_fraction=0.8, bagging_freq=2, min_child_samples=23, min_child_weight=0.001, \n",
    "                       min_split_gain=None, reg_lambda=0.3, reg_alpha=0.5, param_grid=lgb_params)     #要调的参数设置成None？\n",
    "print('time: ', datetime.datetime.fromtimestamp(time()-time0).strftime(\"%M:%S:%f\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7.降低learning_rate\n",
    "\n",
    "之前使用较高的学习速率是因为可以让收敛更快，。最后我们使用较低的学习速率，以及使用更多的决策树n_estimators来训练数据，看能不能可以进一步的优化分数。可以用回lightGBM的cv函数了 ，代入之前优化好的参数。\n",
    "\n",
    "将learning_rate调小的效果还是很明显的，上一步调参auc的最优结果是0.735752，这里到7300步时已达到这个效果；在12650步之后，就基本每次只增加<0.000005了。\n",
    "\n",
    "auc一直在增加，为什么？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "params = {'objective' : 'binary',\n",
    "         'metric' : 'auc',\n",
    "         'max_depth' : 5,\n",
    "         'num_leaves' : 31,\n",
    "          \n",
    "         'learning_rate' : 0.005,\n",
    "          \n",
    "         'feature_fraction' : 0.8,\n",
    "         'bagging_fraction' : 0.8,\n",
    "         'bagging_freq': 2, \n",
    "          \n",
    "         'min_child_samples' : 23,     \n",
    "         'min_child_weight': 0.001,    \n",
    "         \n",
    "         'reg_alpha': 0.5,      \n",
    "         'reg_lambda' : 0.3,     \n",
    "         'min_split_gain' : 0.0,\n",
    "         'n_jobs' : -1\n",
    "         }\n",
    "\n",
    "lgb_data_train = lgb.Dataset(x_train, y_train, silent=True)\n",
    "\n",
    "time0 = time()\n",
    "cv_results = lgb.cv(\n",
    "    params, lgb_data_train, num_boost_round=50000, nfold=5, stratified=False, shuffle=True, metrics='auc',\n",
    "    early_stopping_rounds=200, verbose_eval=200, show_stdv=True, seed=0)\n",
    "print(datetime.datetime.fromtimestamp(time()-time0).strftime(\"%M:%S:%f\"))\n",
    "\n",
    "print('best n_estimators:', len(cv_results['auc-mean']))\n",
    "print('best cv score:', cv_results['auc-mean'][-1])\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "确定好参数后，是要代入全部的训练数据来训练最终的模型？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************************ 1 ************************************\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[500]\ttraining's auc: 0.729581\tvalid_1's auc: 0.727319\n",
      "[1000]\ttraining's auc: 0.736593\tvalid_1's auc: 0.731597\n",
      "[1500]\ttraining's auc: 0.741263\tvalid_1's auc: 0.733631\n",
      "[2000]\ttraining's auc: 0.745084\tvalid_1's auc: 0.734973\n",
      "[2500]\ttraining's auc: 0.748338\tvalid_1's auc: 0.735933\n",
      "[3000]\ttraining's auc: 0.751314\tvalid_1's auc: 0.736646\n",
      "[3500]\ttraining's auc: 0.754162\tvalid_1's auc: 0.73718\n",
      "[4000]\ttraining's auc: 0.756799\tvalid_1's auc: 0.737558\n",
      "[4500]\ttraining's auc: 0.759248\tvalid_1's auc: 0.737894\n",
      "[5000]\ttraining's auc: 0.761683\tvalid_1's auc: 0.738199\n",
      "[5500]\ttraining's auc: 0.763988\tvalid_1's auc: 0.738432\n",
      "[6000]\ttraining's auc: 0.766212\tvalid_1's auc: 0.738604\n",
      "[6500]\ttraining's auc: 0.768361\tvalid_1's auc: 0.738735\n",
      "[7000]\ttraining's auc: 0.770535\tvalid_1's auc: 0.738872\n",
      "[7500]\ttraining's auc: 0.772686\tvalid_1's auc: 0.738981\n",
      "[8000]\ttraining's auc: 0.774713\tvalid_1's auc: 0.739049\n",
      "[8500]\ttraining's auc: 0.776692\tvalid_1's auc: 0.739093\n",
      "Early stopping, best iteration is:\n",
      "[8749]\ttraining's auc: 0.777716\tvalid_1's auc: 0.739122\n",
      "[0.7391223235225274]\n",
      "************************************ 2 ************************************\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[500]\ttraining's auc: 0.729432\tvalid_1's auc: 0.727215\n",
      "[1000]\ttraining's auc: 0.736499\tvalid_1's auc: 0.7317\n",
      "[1500]\ttraining's auc: 0.741164\tvalid_1's auc: 0.733813\n",
      "[2000]\ttraining's auc: 0.744925\tvalid_1's auc: 0.735126\n",
      "[2500]\ttraining's auc: 0.748221\tvalid_1's auc: 0.736059\n",
      "[3000]\ttraining's auc: 0.751306\tvalid_1's auc: 0.73675\n",
      "[3500]\ttraining's auc: 0.754122\tvalid_1's auc: 0.73728\n",
      "[4000]\ttraining's auc: 0.756735\tvalid_1's auc: 0.737671\n",
      "[4500]\ttraining's auc: 0.759224\tvalid_1's auc: 0.737963\n",
      "[5000]\ttraining's auc: 0.761632\tvalid_1's auc: 0.738219\n",
      "[5500]\ttraining's auc: 0.76398\tvalid_1's auc: 0.738418\n",
      "[6000]\ttraining's auc: 0.766249\tvalid_1's auc: 0.738584\n",
      "[6500]\ttraining's auc: 0.7685\tvalid_1's auc: 0.738727\n",
      "[7000]\ttraining's auc: 0.770674\tvalid_1's auc: 0.738887\n",
      "[7500]\ttraining's auc: 0.772771\tvalid_1's auc: 0.73895\n",
      "[8000]\ttraining's auc: 0.774793\tvalid_1's auc: 0.738998\n",
      "[8500]\ttraining's auc: 0.776826\tvalid_1's auc: 0.739042\n",
      "Early stopping, best iteration is:\n",
      "[8421]\ttraining's auc: 0.776522\tvalid_1's auc: 0.739049\n",
      "[0.7391223235225274, 0.7390489133058429]\n",
      "************************************ 3 ************************************\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[500]\ttraining's auc: 0.730566\tvalid_1's auc: 0.723627\n",
      "[1000]\ttraining's auc: 0.737577\tvalid_1's auc: 0.727875\n",
      "[1500]\ttraining's auc: 0.742256\tvalid_1's auc: 0.729933\n",
      "[2000]\ttraining's auc: 0.746038\tvalid_1's auc: 0.731248\n",
      "[2500]\ttraining's auc: 0.749376\tvalid_1's auc: 0.732185\n",
      "[3000]\ttraining's auc: 0.752402\tvalid_1's auc: 0.732844\n",
      "[3500]\ttraining's auc: 0.755077\tvalid_1's auc: 0.733351\n",
      "[4000]\ttraining's auc: 0.757689\tvalid_1's auc: 0.733762\n",
      "[4500]\ttraining's auc: 0.760137\tvalid_1's auc: 0.734077\n",
      "[5000]\ttraining's auc: 0.762502\tvalid_1's auc: 0.73438\n",
      "[5500]\ttraining's auc: 0.764856\tvalid_1's auc: 0.734627\n",
      "[6000]\ttraining's auc: 0.767054\tvalid_1's auc: 0.734773\n",
      "[6500]\ttraining's auc: 0.769214\tvalid_1's auc: 0.73491\n",
      "[7000]\ttraining's auc: 0.771356\tvalid_1's auc: 0.735054\n",
      "[7500]\ttraining's auc: 0.773458\tvalid_1's auc: 0.735163\n",
      "[8000]\ttraining's auc: 0.775467\tvalid_1's auc: 0.735283\n",
      "[8500]\ttraining's auc: 0.777428\tvalid_1's auc: 0.735332\n",
      "[9000]\ttraining's auc: 0.779413\tvalid_1's auc: 0.735382\n",
      "[9500]\ttraining's auc: 0.781327\tvalid_1's auc: 0.735426\n",
      "[10000]\ttraining's auc: 0.783257\tvalid_1's auc: 0.735446\n",
      "Early stopping, best iteration is:\n",
      "[9958]\ttraining's auc: 0.783094\tvalid_1's auc: 0.735457\n",
      "[0.7391223235225274, 0.7390489133058429, 0.7354565953757946]\n",
      "************************************ 4 ************************************\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[500]\ttraining's auc: 0.729837\tvalid_1's auc: 0.726524\n",
      "[1000]\ttraining's auc: 0.736843\tvalid_1's auc: 0.730675\n",
      "[1500]\ttraining's auc: 0.741395\tvalid_1's auc: 0.732714\n",
      "[2000]\ttraining's auc: 0.745311\tvalid_1's auc: 0.734183\n",
      "[2500]\ttraining's auc: 0.748559\tvalid_1's auc: 0.735124\n",
      "[3000]\ttraining's auc: 0.751542\tvalid_1's auc: 0.735829\n",
      "[3500]\ttraining's auc: 0.754347\tvalid_1's auc: 0.736455\n",
      "[4000]\ttraining's auc: 0.756976\tvalid_1's auc: 0.736934\n",
      "[4500]\ttraining's auc: 0.759477\tvalid_1's auc: 0.737302\n",
      "[5000]\ttraining's auc: 0.761789\tvalid_1's auc: 0.737568\n",
      "[5500]\ttraining's auc: 0.763984\tvalid_1's auc: 0.737752\n",
      "[6000]\ttraining's auc: 0.766273\tvalid_1's auc: 0.73798\n",
      "[6500]\ttraining's auc: 0.768402\tvalid_1's auc: 0.73812\n",
      "[7000]\ttraining's auc: 0.770502\tvalid_1's auc: 0.73823\n",
      "[7500]\ttraining's auc: 0.77261\tvalid_1's auc: 0.738339\n",
      "[8000]\ttraining's auc: 0.774663\tvalid_1's auc: 0.738427\n",
      "[8500]\ttraining's auc: 0.776636\tvalid_1's auc: 0.738496\n",
      "[9000]\ttraining's auc: 0.778633\tvalid_1's auc: 0.738568\n",
      "[9500]\ttraining's auc: 0.780573\tvalid_1's auc: 0.738647\n",
      "Early stopping, best iteration is:\n",
      "[9588]\ttraining's auc: 0.780915\tvalid_1's auc: 0.738666\n",
      "[0.7391223235225274, 0.7390489133058429, 0.7354565953757946, 0.7386663803664781]\n",
      "************************************ 5 ************************************\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[500]\ttraining's auc: 0.729767\tvalid_1's auc: 0.726064\n",
      "[1000]\ttraining's auc: 0.736649\tvalid_1's auc: 0.730385\n",
      "[1500]\ttraining's auc: 0.741346\tvalid_1's auc: 0.732568\n",
      "[2000]\ttraining's auc: 0.745244\tvalid_1's auc: 0.733993\n",
      "[2500]\ttraining's auc: 0.748524\tvalid_1's auc: 0.734865\n",
      "[3000]\ttraining's auc: 0.751501\tvalid_1's auc: 0.735497\n",
      "[3500]\ttraining's auc: 0.754239\tvalid_1's auc: 0.735991\n",
      "[4000]\ttraining's auc: 0.756791\tvalid_1's auc: 0.736384\n",
      "[4500]\ttraining's auc: 0.759255\tvalid_1's auc: 0.736687\n",
      "[5000]\ttraining's auc: 0.761624\tvalid_1's auc: 0.736957\n",
      "[5500]\ttraining's auc: 0.763905\tvalid_1's auc: 0.737175\n",
      "[6000]\ttraining's auc: 0.766153\tvalid_1's auc: 0.73734\n",
      "[6500]\ttraining's auc: 0.76838\tvalid_1's auc: 0.737485\n",
      "[7000]\ttraining's auc: 0.770518\tvalid_1's auc: 0.737583\n",
      "[7500]\ttraining's auc: 0.77265\tvalid_1's auc: 0.737699\n",
      "[8000]\ttraining's auc: 0.774701\tvalid_1's auc: 0.737793\n",
      "Early stopping, best iteration is:\n",
      "[8118]\ttraining's auc: 0.775166\tvalid_1's auc: 0.737813\n",
      "[0.7391223235225274, 0.7390489133058429, 0.7354565953757946, 0.7386663803664781, 0.7378130937321666]\n",
      "lgb_scotrainre_list:[0.7391223235225274, 0.7390489133058429, 0.7354565953757946, 0.7386663803664781, 0.7378130937321666]\n",
      "lgb_score_mean:0.7380214612605619\n",
      "lgb_score_std:0.0013642162667319693\n"
     ]
    }
   ],
   "source": [
    "test = np.zeros(x_test.shape[0])\n",
    "test_pred = np.zeros(x_test.shape[0])\n",
    "\n",
    "cv_scores = []\n",
    "for i, (train_index, valid_index) in enumerate(kf.split(x_train, y_train)):\n",
    "    print('************************************ {} ************************************'.format(str(i+1)))\n",
    "    X_train_split, y_train_split, X_val, y_val = x_train.iloc[train_index], y_train[train_index], x_train.iloc[valid_index], y_train[valid_index]\n",
    "    \n",
    "    train_matrix = lgb.Dataset(X_train_split, label=y_train_split)\n",
    "    valid_matrix = lgb.Dataset(X_val, label=y_val)\n",
    "    #test_matrix = xgb.DMatrix(x_test)\n",
    "\n",
    "    params = {'objective' : 'binary',\n",
    "         'metric' : 'auc',\n",
    "         'max_depth' : 5,\n",
    "         'num_leaves' : 31,\n",
    "          \n",
    "         'learning_rate' : 0.01,    #0.005\n",
    "          \n",
    "         'feature_fraction' : 0.8,\n",
    "         'bagging_fraction' : 0.8,\n",
    "         'bagging_freq': 2, \n",
    "          \n",
    "         'min_child_samples' : 23,     \n",
    "         'min_child_weight': 0.001,    \n",
    "         \n",
    "         'reg_alpha': 0.5,      \n",
    "         'reg_lambda' : 0.3,     \n",
    "         'min_split_gain' : 0.0,\n",
    "         'n_jobs' : -1\n",
    "         }\n",
    "    #watchlist = [(train_matrix, 'train'),(valid_matrix, 'eval')]\n",
    "    \n",
    "    model = lgb.train(params, train_matrix, 25000, valid_sets=[train_matrix, valid_matrix], verbose_eval=100, early_stopping_rounds=200)\n",
    "    val_pred = model.predict(X_val, num_iteration=model.best_iteration)\n",
    "    test_pred += model.predict(x_test, num_iteration=model.best_iteration)\n",
    "    \n",
    "    #train[valid_index] = val_pred\n",
    "    #test = test_pred / kf.n_splits\n",
    "    cv_scores.append(roc_auc_score(y_val, val_pred))\n",
    "    print(cv_scores)\n",
    "\n",
    "print(\"lgb_scotrainre_list:{}\".format(cv_scores))\n",
    "print(\"lgb_score_mean:{}\".format(np.mean(cv_scores)))\n",
    "print(\"lgb_score_std:{}\".format(np.std(cv_scores)))\n",
    "\n",
    "#还是有很强的过拟合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.34949527, 1.54133269, 3.22277792, ..., 0.65466506, 1.32097488,\n",
       "       0.12891482])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test_pred / 5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test['isDefault'] = test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test[['id','isDefault']].to_csv('test_sub-09-23-lgb.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "提交之后发现线上分数只比用baseline的参数涨了0.0005，啊这。。。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:sklearn]",
   "language": "python",
   "name": "conda-env-sklearn-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
