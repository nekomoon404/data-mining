{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task05 GBDT+LR\n",
    "\n",
    "### 1. GBDT+LR模型\n",
    "\n",
    "FM、FFM模型只能做二阶的特征交叉，如果继续提高特征交叉的维度，会不可避免地产生组合爆炸和筛选的问题。Facebook提出了GBDT+LR组合模型的解决方案。利用GBDT自动进行特征筛选和组合，进而生成新的离散特征向量，再把该特征向量当做LR模型输入。\n",
    "\n",
    "#### 1.1.逻辑回归模型\n",
    "\n",
    "Logistic Regression的原理比较熟悉了，这里就不赘述了。LR应用到推荐系统中的优势与局限性：\n",
    "\n",
    "**优点：**\n",
    "\n",
    "1. LR模型形式简单，可解释性好，从特征的权重可以看到不同的特征对最后结果的影响。\n",
    "2. 训练时便于并行化，在预测时只需要对特征进行线性加权，所以性能比较好，往往适合处理海量id类特征，用id类特征有一个很重要的好处，就是防止信息损失（相对于范化的 CTR 特征），对于头部资源会有更细致的描述。\n",
    "3. 资源占用小,尤其是内存。在实际的工程应用中只需要存储权重比较大的特征及特征对应的权重。\n",
    "4. 方便输出结果调整，调整判定分类的阈值。\n",
    "\n",
    "**局限性：**\n",
    "\n",
    "1. 表达能力不强， 无法进行特征交叉， 特征筛选等一系列“高级“操作（需要有经验的工程师来做）， 因此可能造成信息的损失。\n",
    "2. 准确率并不是很高，模型较简单。\n",
    "3. 处理非线性数据较麻烦。逻辑回归在不引入其他方法的情况下，只能处理线性可分的数据， 如果想处理非线性， 首先对连续特征的处理需要先进行离散化（离散化的目的是为了引入非线性）。\n",
    "4. 模型迁移起来比较困难，换一个领域又需要重新进行大量的特征工程。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2. GBDT模型\n",
    "\n",
    "GBDT（Gradient Boosting Decision Tree）梯度提升树，属于Boosting一派的集成学习模型。GBDT通过采用加法模型，即基函数的线性组合，以及不断减小训练过程产生的误差来达到数据分类或者回归。\n",
    "\n",
    "gbdt通过多轮迭代， 每轮迭代会产生一个弱分类器， 每个分类器在上一轮分类器的残差基础上进行训练gbdt对弱分类器的要求一般是足够简单， 并且低方差高偏差。 因为训练的过程是通过降低偏差来不断提高最终分类器的精度。 由于上述高偏差和简单的要求，每个分类回归树的深度不会很深。最终的总分类器是将每轮训练得到的弱分类器加权求和得到的。\n",
    "\n",
    "gbdt每轮的训练是在上一轮的训练的残差基础之上进行训练的， 而这里的残差指的就是当前模型的负梯度值，\n",
    "这个就要求每轮迭代的时候，弱分类器的输出的结果相减是有意义的， 而gbdt无论用于分类还是回归一直都是使用的CART回归树。\n",
    "\n",
    "**GBDT的优点：** 可以把树的生成过程理解成自动进行多维度的特征组合的过程，从根结点到叶子节点上的整个路径(多个特征值判断)，才能最终决定一棵树的预测值， 另外，对于连续型特征的处理，GBDT 可以拆分出一个临界阈值，比如大于 0.027 走左子树，小于等于 0.027（或者 default 值）走右子树，这样很好的规避了人工离散化的问题。这样就非常轻松的解决了逻辑回归那里自动发现特征并进行有效组合的问题， 这也是GBDT的优势所在。\n",
    "\n",
    "**局限性：** 对于海量的id类特征，GBDT 由于树的深度和棵树限制（防止过拟合），不能有效的存储；另外海量特征在也会存在性能瓶颈，当 GBDT 的 one hot 特征大于 10 万维时，就必须做分布式的训练才能保证不爆内\n",
    "存。所以 GBDT 通常配合少量的反馈 CTR 特征来表达，这样虽然具有一定的范化能力，但是同时会有信息损失，对于头部资源不能有效的表达。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 GBDT+LR模型\n",
    "\n",
    "GBDT+LR组合模型利用GBDT自动进行特征筛选和组合，进而生成新的离散特征向量，再把该特征向量当做LR模型输入。有几个关键点需要注意：\n",
    "\n",
    "1. 通过GBDT进行特征组合之后得到的离散向量是和训练数据的原特征一块作为逻辑回归的输入， 而不仅仅全是这种离散特征。\n",
    "\n",
    "2. 建树的时候用ensemble建树的原因就是一棵树的表达能力很弱，不足以表达多个有区分性的特征组合，多棵树的表达能力更强一些。GBDT每棵树都在学习前面棵树尚存的不足，迭代多少次就会生成多少棵树。\n",
    "\n",
    "3. RF也是多棵树，但从效果上有实践证明不如GBDT。且GBDT前面的树，特征分裂主要体现对多数样本有区分度的特征；后面的树，主要体现的是经过前N颗树，残差仍然较大的少数样本。优先选用在整体上有区分度的特征，再选用针对少数样本有区分度的特征，思路更加合理，这应该也是用GBDT的原因。\n",
    "\n",
    "4. 在CRT预估中， GBDT一般会建立两类树(非ID特征建一类， ID类特征建一类)， AD，ID类特征在CTR预估中是非常重要的特征，直接将AD，ID作为feature进行建树不可行，故考虑为每个AD，ID建GBDT树。\n",
    "  1. 非ID类树：不以细粒度的ID建树，此类树作为base，即便曝光少的广告、广告主，仍可以通过此类树得到有区分性的特征、特征组合\n",
    "  2. ID类树：以细粒度 的ID建一类树，用于发现曝光充分的ID对应有区分性的特征、特征组合"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.代码实现\n",
    "\n",
    "GBDT部分基于Lightgbm实现，所用的数据集是criteo数据集的一部分，有train.csv和test.csv两个文件：\n",
    "\n",
    "* train.csv： 训练集由Criteo 7天内的部分流量组成。每一行对应一个由Criteo提供的显示广告。为了减少数据集的大小，正(点击)和负(未点击)的例子都以不同的比例进行了抽样。示例是按时间顺序排列的;\n",
    "\n",
    "* test.csv: 测试集的计算方法与训练集相同，只是针对训练期之后一天的事件。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import lightgbm as lgb\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "import gc\n",
    "from scipy import sparse\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义column名称\n",
    "cols = ['label',  'I1', 'I2', 'I3', 'I4', 'I5', 'I6', 'I7', 'I8', 'I9',\n",
    "       'I10', 'I11', 'I12', 'I13', 'C1', 'C2', 'C3', 'C4', 'C5', 'C6',\n",
    "       'C7', 'C8', 'C9', 'C10', 'C11', 'C12', 'C13', 'C14', 'C15', 'C16',\n",
    "       'C17', 'C18', 'C19', 'C20', 'C21', 'C22', 'C23', 'C24', 'C25',\n",
    "       'C26']\n",
    "\n",
    "data = pd.read_table('G:\\Datasets\\dac_sample.tar\\dac_sample.txt', header=None, names=cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义特征组\n",
    "dense_feats = [f for f in cols if f[0] == \"I\"]\n",
    "sparse_feats = [f for f in cols if f[0] == \"C\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#处理dense特征\n",
    "def process_dense_feats(data, feats):\n",
    "    d = data.copy()\n",
    "    d = d[feats].fillna(0.0)   #缺失值填充为0\n",
    "    for f in feats:\n",
    "        d[f] = d[f].apply(lambda x : np.log(x + 1) if x > -1 else -1)\n",
    "        \n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dense = process_dense_feats(data, dense_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#处理sparse特征\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def process_sparse_feats(data, feats):\n",
    "    d = data.copy()\n",
    "    d = d[feats].fillna(\"-1\")   #缺失值填充为-1\n",
    "    for f in feats:\n",
    "        label_encoder = LabelEncoder()    #这里使用了labelEnconder\n",
    "        d[f] = label_encoder.fit_transform(d[f])\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sparse = process_sparse_feats(data, sparse_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 40)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#处理后的全量数据\n",
    "data_df = pd.concat([data_dense, data_sparse], axis=1)\n",
    "data_df['label'] = data['label']\n",
    "\n",
    "data_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#划分训练集和测试集\n",
    "train, test = train_test_split(data_df, test_size=0.2,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train.pop('label')\n",
    "x_train = train\n",
    "\n",
    "y_val = test.pop('label')\n",
    "x_val = test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GBDT建模"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbm = lgb.LGBMClassifier(boosting_type='gbdt',  # 这里用gbdt\n",
    "                             objective='binary', \n",
    "                             subsample=0.8,\n",
    "                             min_child_weight=0.5, \n",
    "                             colsample_bytree=0.7,\n",
    "                             num_leaves=50,\n",
    "                             max_depth=6,\n",
    "                             learning_rate=0.1,\n",
    "                             n_estimators=1000,\n",
    "                             silent=True,\n",
    "                             \n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\ttrain's binary_logloss: 0.52386\tval's binary_logloss: 0.524533\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[2]\ttrain's binary_logloss: 0.514941\tval's binary_logloss: 0.515807\n",
      "[3]\ttrain's binary_logloss: 0.507577\tval's binary_logloss: 0.50901\n",
      "[4]\ttrain's binary_logloss: 0.501155\tval's binary_logloss: 0.502763\n",
      "[5]\ttrain's binary_logloss: 0.496005\tval's binary_logloss: 0.497826\n",
      "[6]\ttrain's binary_logloss: 0.491345\tval's binary_logloss: 0.493613\n",
      "[7]\ttrain's binary_logloss: 0.487444\tval's binary_logloss: 0.490113\n",
      "[8]\ttrain's binary_logloss: 0.484134\tval's binary_logloss: 0.487229\n",
      "[9]\ttrain's binary_logloss: 0.481008\tval's binary_logloss: 0.484421\n",
      "[10]\ttrain's binary_logloss: 0.478329\tval's binary_logloss: 0.482231\n",
      "[11]\ttrain's binary_logloss: 0.475974\tval's binary_logloss: 0.480209\n",
      "[12]\ttrain's binary_logloss: 0.473863\tval's binary_logloss: 0.478519\n",
      "[13]\ttrain's binary_logloss: 0.47174\tval's binary_logloss: 0.476791\n",
      "[14]\ttrain's binary_logloss: 0.469873\tval's binary_logloss: 0.475294\n",
      "[15]\ttrain's binary_logloss: 0.468105\tval's binary_logloss: 0.473914\n",
      "[16]\ttrain's binary_logloss: 0.466558\tval's binary_logloss: 0.47273\n",
      "[17]\ttrain's binary_logloss: 0.465204\tval's binary_logloss: 0.471698\n",
      "[18]\ttrain's binary_logloss: 0.463811\tval's binary_logloss: 0.47076\n",
      "[19]\ttrain's binary_logloss: 0.462351\tval's binary_logloss: 0.46984\n",
      "[20]\ttrain's binary_logloss: 0.461112\tval's binary_logloss: 0.468956\n",
      "[21]\ttrain's binary_logloss: 0.459749\tval's binary_logloss: 0.468113\n",
      "[22]\ttrain's binary_logloss: 0.458478\tval's binary_logloss: 0.467433\n",
      "[23]\ttrain's binary_logloss: 0.457331\tval's binary_logloss: 0.466693\n",
      "[24]\ttrain's binary_logloss: 0.456274\tval's binary_logloss: 0.466148\n",
      "[25]\ttrain's binary_logloss: 0.455206\tval's binary_logloss: 0.46551\n",
      "[26]\ttrain's binary_logloss: 0.454292\tval's binary_logloss: 0.464986\n",
      "[27]\ttrain's binary_logloss: 0.453442\tval's binary_logloss: 0.464522\n",
      "[28]\ttrain's binary_logloss: 0.452596\tval's binary_logloss: 0.464079\n",
      "[29]\ttrain's binary_logloss: 0.451723\tval's binary_logloss: 0.46352\n",
      "[30]\ttrain's binary_logloss: 0.450788\tval's binary_logloss: 0.462922\n",
      "[31]\ttrain's binary_logloss: 0.449897\tval's binary_logloss: 0.462461\n",
      "[32]\ttrain's binary_logloss: 0.449007\tval's binary_logloss: 0.462011\n",
      "[33]\ttrain's binary_logloss: 0.448324\tval's binary_logloss: 0.461717\n",
      "[34]\ttrain's binary_logloss: 0.447598\tval's binary_logloss: 0.461317\n",
      "[35]\ttrain's binary_logloss: 0.44671\tval's binary_logloss: 0.460778\n",
      "[36]\ttrain's binary_logloss: 0.445799\tval's binary_logloss: 0.460228\n",
      "[37]\ttrain's binary_logloss: 0.44488\tval's binary_logloss: 0.459618\n",
      "[38]\ttrain's binary_logloss: 0.444187\tval's binary_logloss: 0.459331\n",
      "[39]\ttrain's binary_logloss: 0.443466\tval's binary_logloss: 0.459093\n",
      "[40]\ttrain's binary_logloss: 0.442984\tval's binary_logloss: 0.458895\n",
      "[41]\ttrain's binary_logloss: 0.442088\tval's binary_logloss: 0.458327\n",
      "[42]\ttrain's binary_logloss: 0.441694\tval's binary_logloss: 0.458222\n",
      "[43]\ttrain's binary_logloss: 0.441053\tval's binary_logloss: 0.457964\n",
      "[44]\ttrain's binary_logloss: 0.440422\tval's binary_logloss: 0.457661\n",
      "[45]\ttrain's binary_logloss: 0.43977\tval's binary_logloss: 0.45742\n",
      "[46]\ttrain's binary_logloss: 0.439177\tval's binary_logloss: 0.457186\n",
      "[47]\ttrain's binary_logloss: 0.438615\tval's binary_logloss: 0.456993\n",
      "[48]\ttrain's binary_logloss: 0.437979\tval's binary_logloss: 0.456567\n",
      "[49]\ttrain's binary_logloss: 0.437257\tval's binary_logloss: 0.456151\n",
      "[50]\ttrain's binary_logloss: 0.436816\tval's binary_logloss: 0.455956\n",
      "[51]\ttrain's binary_logloss: 0.436213\tval's binary_logloss: 0.455694\n",
      "[52]\ttrain's binary_logloss: 0.435547\tval's binary_logloss: 0.455443\n",
      "[53]\ttrain's binary_logloss: 0.435114\tval's binary_logloss: 0.455255\n",
      "[54]\ttrain's binary_logloss: 0.434554\tval's binary_logloss: 0.455048\n",
      "[55]\ttrain's binary_logloss: 0.434202\tval's binary_logloss: 0.454948\n",
      "[56]\ttrain's binary_logloss: 0.433883\tval's binary_logloss: 0.45483\n",
      "[57]\ttrain's binary_logloss: 0.433207\tval's binary_logloss: 0.454411\n",
      "[58]\ttrain's binary_logloss: 0.432727\tval's binary_logloss: 0.454162\n",
      "[59]\ttrain's binary_logloss: 0.432335\tval's binary_logloss: 0.45408\n",
      "[60]\ttrain's binary_logloss: 0.431804\tval's binary_logloss: 0.454088\n",
      "[61]\ttrain's binary_logloss: 0.43147\tval's binary_logloss: 0.454011\n",
      "[62]\ttrain's binary_logloss: 0.431058\tval's binary_logloss: 0.453859\n",
      "[63]\ttrain's binary_logloss: 0.430589\tval's binary_logloss: 0.453738\n",
      "[64]\ttrain's binary_logloss: 0.430408\tval's binary_logloss: 0.453697\n",
      "[65]\ttrain's binary_logloss: 0.429852\tval's binary_logloss: 0.453557\n",
      "[66]\ttrain's binary_logloss: 0.429447\tval's binary_logloss: 0.453486\n",
      "[67]\ttrain's binary_logloss: 0.42881\tval's binary_logloss: 0.453208\n",
      "[68]\ttrain's binary_logloss: 0.428315\tval's binary_logloss: 0.453002\n",
      "[69]\ttrain's binary_logloss: 0.427894\tval's binary_logloss: 0.452945\n",
      "[70]\ttrain's binary_logloss: 0.427385\tval's binary_logloss: 0.452918\n",
      "[71]\ttrain's binary_logloss: 0.426971\tval's binary_logloss: 0.452859\n",
      "[72]\ttrain's binary_logloss: 0.426589\tval's binary_logloss: 0.452685\n",
      "[73]\ttrain's binary_logloss: 0.42602\tval's binary_logloss: 0.452634\n",
      "[74]\ttrain's binary_logloss: 0.425781\tval's binary_logloss: 0.452567\n",
      "[75]\ttrain's binary_logloss: 0.425495\tval's binary_logloss: 0.452525\n",
      "[76]\ttrain's binary_logloss: 0.424986\tval's binary_logloss: 0.452367\n",
      "[77]\ttrain's binary_logloss: 0.424497\tval's binary_logloss: 0.452242\n",
      "[78]\ttrain's binary_logloss: 0.424224\tval's binary_logloss: 0.452192\n",
      "[79]\ttrain's binary_logloss: 0.423742\tval's binary_logloss: 0.452063\n",
      "[80]\ttrain's binary_logloss: 0.423237\tval's binary_logloss: 0.451958\n",
      "[81]\ttrain's binary_logloss: 0.422685\tval's binary_logloss: 0.451845\n",
      "[82]\ttrain's binary_logloss: 0.422165\tval's binary_logloss: 0.451752\n",
      "[83]\ttrain's binary_logloss: 0.421672\tval's binary_logloss: 0.451597\n",
      "[84]\ttrain's binary_logloss: 0.421278\tval's binary_logloss: 0.451469\n",
      "[85]\ttrain's binary_logloss: 0.420838\tval's binary_logloss: 0.45146\n",
      "[86]\ttrain's binary_logloss: 0.420617\tval's binary_logloss: 0.451479\n",
      "[87]\ttrain's binary_logloss: 0.420377\tval's binary_logloss: 0.45143\n",
      "[88]\ttrain's binary_logloss: 0.41993\tval's binary_logloss: 0.451385\n",
      "[89]\ttrain's binary_logloss: 0.419684\tval's binary_logloss: 0.451357\n",
      "[90]\ttrain's binary_logloss: 0.419427\tval's binary_logloss: 0.451312\n",
      "[91]\ttrain's binary_logloss: 0.418888\tval's binary_logloss: 0.451137\n",
      "[92]\ttrain's binary_logloss: 0.41856\tval's binary_logloss: 0.451194\n",
      "[93]\ttrain's binary_logloss: 0.418391\tval's binary_logloss: 0.451168\n",
      "[94]\ttrain's binary_logloss: 0.418251\tval's binary_logloss: 0.451153\n",
      "[95]\ttrain's binary_logloss: 0.417762\tval's binary_logloss: 0.451153\n",
      "[96]\ttrain's binary_logloss: 0.417352\tval's binary_logloss: 0.451099\n",
      "[97]\ttrain's binary_logloss: 0.417252\tval's binary_logloss: 0.451051\n",
      "[98]\ttrain's binary_logloss: 0.416987\tval's binary_logloss: 0.45097\n",
      "[99]\ttrain's binary_logloss: 0.416711\tval's binary_logloss: 0.450889\n",
      "[100]\ttrain's binary_logloss: 0.416308\tval's binary_logloss: 0.45076\n",
      "[101]\ttrain's binary_logloss: 0.415909\tval's binary_logloss: 0.450697\n",
      "[102]\ttrain's binary_logloss: 0.41553\tval's binary_logloss: 0.450562\n",
      "[103]\ttrain's binary_logloss: 0.415171\tval's binary_logloss: 0.450576\n",
      "[104]\ttrain's binary_logloss: 0.414811\tval's binary_logloss: 0.450499\n",
      "[105]\ttrain's binary_logloss: 0.414491\tval's binary_logloss: 0.450448\n",
      "[106]\ttrain's binary_logloss: 0.414365\tval's binary_logloss: 0.450422\n",
      "[107]\ttrain's binary_logloss: 0.414112\tval's binary_logloss: 0.450378\n",
      "[108]\ttrain's binary_logloss: 0.414006\tval's binary_logloss: 0.450382\n",
      "[109]\ttrain's binary_logloss: 0.41362\tval's binary_logloss: 0.450311\n",
      "[110]\ttrain's binary_logloss: 0.413105\tval's binary_logloss: 0.450185\n",
      "[111]\ttrain's binary_logloss: 0.412794\tval's binary_logloss: 0.450131\n",
      "[112]\ttrain's binary_logloss: 0.412531\tval's binary_logloss: 0.450068\n",
      "[113]\ttrain's binary_logloss: 0.412136\tval's binary_logloss: 0.450088\n",
      "[114]\ttrain's binary_logloss: 0.411836\tval's binary_logloss: 0.450046\n",
      "[115]\ttrain's binary_logloss: 0.41156\tval's binary_logloss: 0.449983\n",
      "[116]\ttrain's binary_logloss: 0.41102\tval's binary_logloss: 0.449853\n",
      "[117]\ttrain's binary_logloss: 0.410826\tval's binary_logloss: 0.449893\n",
      "[118]\ttrain's binary_logloss: 0.410503\tval's binary_logloss: 0.449878\n",
      "[119]\ttrain's binary_logloss: 0.410053\tval's binary_logloss: 0.449775\n",
      "[120]\ttrain's binary_logloss: 0.40951\tval's binary_logloss: 0.449641\n",
      "[121]\ttrain's binary_logloss: 0.409272\tval's binary_logloss: 0.449621\n",
      "[122]\ttrain's binary_logloss: 0.409138\tval's binary_logloss: 0.449641\n",
      "[123]\ttrain's binary_logloss: 0.408681\tval's binary_logloss: 0.449477\n",
      "[124]\ttrain's binary_logloss: 0.408353\tval's binary_logloss: 0.449396\n",
      "[125]\ttrain's binary_logloss: 0.407895\tval's binary_logloss: 0.449268\n",
      "[126]\ttrain's binary_logloss: 0.407673\tval's binary_logloss: 0.449283\n",
      "[127]\ttrain's binary_logloss: 0.407379\tval's binary_logloss: 0.449301\n",
      "[128]\ttrain's binary_logloss: 0.406894\tval's binary_logloss: 0.449176\n",
      "[129]\ttrain's binary_logloss: 0.406647\tval's binary_logloss: 0.449049\n",
      "[130]\ttrain's binary_logloss: 0.406444\tval's binary_logloss: 0.448979\n",
      "[131]\ttrain's binary_logloss: 0.406284\tval's binary_logloss: 0.448954\n",
      "[132]\ttrain's binary_logloss: 0.405975\tval's binary_logloss: 0.448932\n",
      "[133]\ttrain's binary_logloss: 0.405547\tval's binary_logloss: 0.448909\n",
      "[134]\ttrain's binary_logloss: 0.405159\tval's binary_logloss: 0.448882\n",
      "[135]\ttrain's binary_logloss: 0.404837\tval's binary_logloss: 0.448808\n",
      "[136]\ttrain's binary_logloss: 0.404665\tval's binary_logloss: 0.448811\n",
      "[137]\ttrain's binary_logloss: 0.404162\tval's binary_logloss: 0.448705\n",
      "[138]\ttrain's binary_logloss: 0.403678\tval's binary_logloss: 0.448551\n",
      "[139]\ttrain's binary_logloss: 0.403422\tval's binary_logloss: 0.448561\n",
      "[140]\ttrain's binary_logloss: 0.403143\tval's binary_logloss: 0.448499\n",
      "[141]\ttrain's binary_logloss: 0.402995\tval's binary_logloss: 0.448502\n",
      "[142]\ttrain's binary_logloss: 0.402704\tval's binary_logloss: 0.448493\n",
      "[143]\ttrain's binary_logloss: 0.402318\tval's binary_logloss: 0.448514\n",
      "[144]\ttrain's binary_logloss: 0.402145\tval's binary_logloss: 0.448482\n",
      "[145]\ttrain's binary_logloss: 0.401917\tval's binary_logloss: 0.448519\n",
      "[146]\ttrain's binary_logloss: 0.401475\tval's binary_logloss: 0.448528\n",
      "[147]\ttrain's binary_logloss: 0.401119\tval's binary_logloss: 0.448397\n",
      "[148]\ttrain's binary_logloss: 0.400942\tval's binary_logloss: 0.448377\n",
      "[149]\ttrain's binary_logloss: 0.400598\tval's binary_logloss: 0.448347\n",
      "[150]\ttrain's binary_logloss: 0.400309\tval's binary_logloss: 0.448371\n",
      "[151]\ttrain's binary_logloss: 0.399953\tval's binary_logloss: 0.448282\n",
      "[152]\ttrain's binary_logloss: 0.399555\tval's binary_logloss: 0.448291\n",
      "[153]\ttrain's binary_logloss: 0.399228\tval's binary_logloss: 0.448294\n",
      "[154]\ttrain's binary_logloss: 0.398786\tval's binary_logloss: 0.448299\n",
      "[155]\ttrain's binary_logloss: 0.398386\tval's binary_logloss: 0.448296\n",
      "[156]\ttrain's binary_logloss: 0.397953\tval's binary_logloss: 0.448198\n",
      "[157]\ttrain's binary_logloss: 0.397627\tval's binary_logloss: 0.448174\n",
      "[158]\ttrain's binary_logloss: 0.397401\tval's binary_logloss: 0.448049\n",
      "[159]\ttrain's binary_logloss: 0.39726\tval's binary_logloss: 0.448011\n",
      "[160]\ttrain's binary_logloss: 0.3971\tval's binary_logloss: 0.448038\n",
      "[161]\ttrain's binary_logloss: 0.396916\tval's binary_logloss: 0.44799\n",
      "[162]\ttrain's binary_logloss: 0.396823\tval's binary_logloss: 0.448009\n",
      "[163]\ttrain's binary_logloss: 0.396566\tval's binary_logloss: 0.447987\n",
      "[164]\ttrain's binary_logloss: 0.396171\tval's binary_logloss: 0.447935\n",
      "[165]\ttrain's binary_logloss: 0.39597\tval's binary_logloss: 0.447948\n",
      "[166]\ttrain's binary_logloss: 0.395527\tval's binary_logloss: 0.447939\n",
      "[167]\ttrain's binary_logloss: 0.395154\tval's binary_logloss: 0.447969\n",
      "[168]\ttrain's binary_logloss: 0.39477\tval's binary_logloss: 0.447886\n",
      "[169]\ttrain's binary_logloss: 0.394624\tval's binary_logloss: 0.447851\n",
      "[170]\ttrain's binary_logloss: 0.394413\tval's binary_logloss: 0.44781\n",
      "[171]\ttrain's binary_logloss: 0.394095\tval's binary_logloss: 0.447863\n",
      "[172]\ttrain's binary_logloss: 0.394011\tval's binary_logloss: 0.447853\n",
      "[173]\ttrain's binary_logloss: 0.393655\tval's binary_logloss: 0.447843\n",
      "[174]\ttrain's binary_logloss: 0.39353\tval's binary_logloss: 0.447865\n",
      "[175]\ttrain's binary_logloss: 0.393096\tval's binary_logloss: 0.447786\n",
      "[176]\ttrain's binary_logloss: 0.392892\tval's binary_logloss: 0.447743\n",
      "[177]\ttrain's binary_logloss: 0.392766\tval's binary_logloss: 0.44773\n",
      "[178]\ttrain's binary_logloss: 0.392318\tval's binary_logloss: 0.447663\n",
      "[179]\ttrain's binary_logloss: 0.392093\tval's binary_logloss: 0.447658\n",
      "[180]\ttrain's binary_logloss: 0.391636\tval's binary_logloss: 0.447617\n",
      "[181]\ttrain's binary_logloss: 0.391287\tval's binary_logloss: 0.447614\n",
      "[182]\ttrain's binary_logloss: 0.390961\tval's binary_logloss: 0.447657\n",
      "[183]\ttrain's binary_logloss: 0.390503\tval's binary_logloss: 0.447679\n",
      "[184]\ttrain's binary_logloss: 0.390452\tval's binary_logloss: 0.447721\n",
      "[185]\ttrain's binary_logloss: 0.390095\tval's binary_logloss: 0.447758\n",
      "[186]\ttrain's binary_logloss: 0.389729\tval's binary_logloss: 0.447689\n",
      "[187]\ttrain's binary_logloss: 0.389369\tval's binary_logloss: 0.447689\n",
      "[188]\ttrain's binary_logloss: 0.388966\tval's binary_logloss: 0.447584\n",
      "[189]\ttrain's binary_logloss: 0.388871\tval's binary_logloss: 0.447606\n",
      "[190]\ttrain's binary_logloss: 0.388618\tval's binary_logloss: 0.447503\n",
      "[191]\ttrain's binary_logloss: 0.388331\tval's binary_logloss: 0.447438\n",
      "[192]\ttrain's binary_logloss: 0.387974\tval's binary_logloss: 0.447521\n",
      "[193]\ttrain's binary_logloss: 0.387764\tval's binary_logloss: 0.447515\n",
      "[194]\ttrain's binary_logloss: 0.387325\tval's binary_logloss: 0.447452\n",
      "[195]\ttrain's binary_logloss: 0.387236\tval's binary_logloss: 0.44746\n",
      "[196]\ttrain's binary_logloss: 0.387022\tval's binary_logloss: 0.447487\n",
      "[197]\ttrain's binary_logloss: 0.386803\tval's binary_logloss: 0.447501\n",
      "[198]\ttrain's binary_logloss: 0.386636\tval's binary_logloss: 0.447475\n",
      "[199]\ttrain's binary_logloss: 0.386255\tval's binary_logloss: 0.447498\n",
      "[200]\ttrain's binary_logloss: 0.385846\tval's binary_logloss: 0.447321\n",
      "[201]\ttrain's binary_logloss: 0.385506\tval's binary_logloss: 0.44724\n",
      "[202]\ttrain's binary_logloss: 0.385418\tval's binary_logloss: 0.447237\n",
      "[203]\ttrain's binary_logloss: 0.385264\tval's binary_logloss: 0.447234\n",
      "[204]\ttrain's binary_logloss: 0.385107\tval's binary_logloss: 0.447239\n",
      "[205]\ttrain's binary_logloss: 0.384744\tval's binary_logloss: 0.447212\n",
      "[206]\ttrain's binary_logloss: 0.384418\tval's binary_logloss: 0.447166\n",
      "[207]\ttrain's binary_logloss: 0.384016\tval's binary_logloss: 0.447156\n",
      "[208]\ttrain's binary_logloss: 0.383893\tval's binary_logloss: 0.447156\n",
      "[209]\ttrain's binary_logloss: 0.383736\tval's binary_logloss: 0.447137\n",
      "[210]\ttrain's binary_logloss: 0.38353\tval's binary_logloss: 0.447154\n",
      "[211]\ttrain's binary_logloss: 0.383175\tval's binary_logloss: 0.447109\n",
      "[212]\ttrain's binary_logloss: 0.38284\tval's binary_logloss: 0.44712\n",
      "[213]\ttrain's binary_logloss: 0.382607\tval's binary_logloss: 0.447152\n",
      "[214]\ttrain's binary_logloss: 0.382219\tval's binary_logloss: 0.447165\n",
      "[215]\ttrain's binary_logloss: 0.382032\tval's binary_logloss: 0.44714\n",
      "[216]\ttrain's binary_logloss: 0.381962\tval's binary_logloss: 0.447153\n",
      "[217]\ttrain's binary_logloss: 0.381808\tval's binary_logloss: 0.447168\n",
      "[218]\ttrain's binary_logloss: 0.381688\tval's binary_logloss: 0.447138\n",
      "[219]\ttrain's binary_logloss: 0.381609\tval's binary_logloss: 0.44714\n",
      "[220]\ttrain's binary_logloss: 0.381408\tval's binary_logloss: 0.447068\n",
      "[221]\ttrain's binary_logloss: 0.381074\tval's binary_logloss: 0.447007\n",
      "[222]\ttrain's binary_logloss: 0.380743\tval's binary_logloss: 0.446987\n",
      "[223]\ttrain's binary_logloss: 0.380436\tval's binary_logloss: 0.446931\n",
      "[224]\ttrain's binary_logloss: 0.380301\tval's binary_logloss: 0.446949\n",
      "[225]\ttrain's binary_logloss: 0.380117\tval's binary_logloss: 0.446968\n",
      "[226]\ttrain's binary_logloss: 0.379841\tval's binary_logloss: 0.446981\n",
      "[227]\ttrain's binary_logloss: 0.379541\tval's binary_logloss: 0.446921\n",
      "[228]\ttrain's binary_logloss: 0.379276\tval's binary_logloss: 0.446957\n",
      "[229]\ttrain's binary_logloss: 0.379029\tval's binary_logloss: 0.447028\n",
      "[230]\ttrain's binary_logloss: 0.378899\tval's binary_logloss: 0.44701\n",
      "[231]\ttrain's binary_logloss: 0.3788\tval's binary_logloss: 0.447025\n",
      "[232]\ttrain's binary_logloss: 0.3786\tval's binary_logloss: 0.447048\n",
      "[233]\ttrain's binary_logloss: 0.378444\tval's binary_logloss: 0.447053\n",
      "[234]\ttrain's binary_logloss: 0.37807\tval's binary_logloss: 0.447071\n",
      "[235]\ttrain's binary_logloss: 0.377748\tval's binary_logloss: 0.447099\n",
      "[236]\ttrain's binary_logloss: 0.377522\tval's binary_logloss: 0.447122\n",
      "[237]\ttrain's binary_logloss: 0.377169\tval's binary_logloss: 0.447075\n",
      "[238]\ttrain's binary_logloss: 0.376988\tval's binary_logloss: 0.447104\n",
      "[239]\ttrain's binary_logloss: 0.376703\tval's binary_logloss: 0.447159\n",
      "[240]\ttrain's binary_logloss: 0.376428\tval's binary_logloss: 0.4472\n",
      "[241]\ttrain's binary_logloss: 0.376055\tval's binary_logloss: 0.447172\n",
      "[242]\ttrain's binary_logloss: 0.375802\tval's binary_logloss: 0.447197\n",
      "[243]\ttrain's binary_logloss: 0.375529\tval's binary_logloss: 0.447208\n",
      "[244]\ttrain's binary_logloss: 0.375375\tval's binary_logloss: 0.447191\n",
      "[245]\ttrain's binary_logloss: 0.375033\tval's binary_logloss: 0.447192\n",
      "[246]\ttrain's binary_logloss: 0.374718\tval's binary_logloss: 0.447143\n",
      "[247]\ttrain's binary_logloss: 0.374545\tval's binary_logloss: 0.447161\n",
      "[248]\ttrain's binary_logloss: 0.37439\tval's binary_logloss: 0.447203\n",
      "[249]\ttrain's binary_logloss: 0.374103\tval's binary_logloss: 0.447165\n",
      "[250]\ttrain's binary_logloss: 0.373851\tval's binary_logloss: 0.447228\n",
      "[251]\ttrain's binary_logloss: 0.373644\tval's binary_logloss: 0.447301\n",
      "[252]\ttrain's binary_logloss: 0.373244\tval's binary_logloss: 0.44724\n",
      "[253]\ttrain's binary_logloss: 0.373062\tval's binary_logloss: 0.447242\n",
      "[254]\ttrain's binary_logloss: 0.372833\tval's binary_logloss: 0.447231\n",
      "[255]\ttrain's binary_logloss: 0.372679\tval's binary_logloss: 0.447217\n",
      "[256]\ttrain's binary_logloss: 0.372439\tval's binary_logloss: 0.447273\n",
      "[257]\ttrain's binary_logloss: 0.372094\tval's binary_logloss: 0.447226\n",
      "[258]\ttrain's binary_logloss: 0.371862\tval's binary_logloss: 0.447173\n",
      "[259]\ttrain's binary_logloss: 0.371565\tval's binary_logloss: 0.447182\n",
      "[260]\ttrain's binary_logloss: 0.37129\tval's binary_logloss: 0.447193\n",
      "[261]\ttrain's binary_logloss: 0.37108\tval's binary_logloss: 0.447301\n",
      "[262]\ttrain's binary_logloss: 0.370704\tval's binary_logloss: 0.44733\n",
      "[263]\ttrain's binary_logloss: 0.370491\tval's binary_logloss: 0.447259\n",
      "[264]\ttrain's binary_logloss: 0.370373\tval's binary_logloss: 0.447272\n",
      "[265]\ttrain's binary_logloss: 0.37003\tval's binary_logloss: 0.447261\n",
      "[266]\ttrain's binary_logloss: 0.369816\tval's binary_logloss: 0.447275\n",
      "[267]\ttrain's binary_logloss: 0.369554\tval's binary_logloss: 0.44729\n",
      "[268]\ttrain's binary_logloss: 0.369274\tval's binary_logloss: 0.447339\n",
      "[269]\ttrain's binary_logloss: 0.368919\tval's binary_logloss: 0.447285\n",
      "[270]\ttrain's binary_logloss: 0.368621\tval's binary_logloss: 0.447226\n",
      "[271]\ttrain's binary_logloss: 0.368363\tval's binary_logloss: 0.447234\n",
      "[272]\ttrain's binary_logloss: 0.36803\tval's binary_logloss: 0.447211\n",
      "[273]\ttrain's binary_logloss: 0.367926\tval's binary_logloss: 0.447202\n",
      "[274]\ttrain's binary_logloss: 0.367538\tval's binary_logloss: 0.447143\n",
      "[275]\ttrain's binary_logloss: 0.367211\tval's binary_logloss: 0.447175\n",
      "[276]\ttrain's binary_logloss: 0.366819\tval's binary_logloss: 0.447197\n",
      "[277]\ttrain's binary_logloss: 0.366566\tval's binary_logloss: 0.447147\n",
      "[278]\ttrain's binary_logloss: 0.36629\tval's binary_logloss: 0.447154\n",
      "[279]\ttrain's binary_logloss: 0.36617\tval's binary_logloss: 0.447176\n",
      "[280]\ttrain's binary_logloss: 0.365881\tval's binary_logloss: 0.447162\n",
      "[281]\ttrain's binary_logloss: 0.365705\tval's binary_logloss: 0.447199\n",
      "[282]\ttrain's binary_logloss: 0.365345\tval's binary_logloss: 0.447214\n",
      "[283]\ttrain's binary_logloss: 0.365016\tval's binary_logloss: 0.447275\n",
      "[284]\ttrain's binary_logloss: 0.364906\tval's binary_logloss: 0.447287\n",
      "[285]\ttrain's binary_logloss: 0.364594\tval's binary_logloss: 0.447213\n",
      "[286]\ttrain's binary_logloss: 0.364493\tval's binary_logloss: 0.4472\n",
      "[287]\ttrain's binary_logloss: 0.364233\tval's binary_logloss: 0.447181\n",
      "[288]\ttrain's binary_logloss: 0.364003\tval's binary_logloss: 0.447213\n",
      "[289]\ttrain's binary_logloss: 0.36376\tval's binary_logloss: 0.447236\n",
      "[290]\ttrain's binary_logloss: 0.36368\tval's binary_logloss: 0.447253\n",
      "[291]\ttrain's binary_logloss: 0.363493\tval's binary_logloss: 0.447252\n",
      "[292]\ttrain's binary_logloss: 0.363245\tval's binary_logloss: 0.447234\n",
      "[293]\ttrain's binary_logloss: 0.363052\tval's binary_logloss: 0.447282\n",
      "[294]\ttrain's binary_logloss: 0.362847\tval's binary_logloss: 0.447283\n",
      "[295]\ttrain's binary_logloss: 0.362571\tval's binary_logloss: 0.447278\n",
      "[296]\ttrain's binary_logloss: 0.362313\tval's binary_logloss: 0.447332\n",
      "[297]\ttrain's binary_logloss: 0.362152\tval's binary_logloss: 0.447357\n",
      "[298]\ttrain's binary_logloss: 0.361887\tval's binary_logloss: 0.447338\n",
      "[299]\ttrain's binary_logloss: 0.361796\tval's binary_logloss: 0.447386\n",
      "[300]\ttrain's binary_logloss: 0.361499\tval's binary_logloss: 0.447375\n",
      "[301]\ttrain's binary_logloss: 0.361421\tval's binary_logloss: 0.447356\n",
      "[302]\ttrain's binary_logloss: 0.361125\tval's binary_logloss: 0.447416\n",
      "[303]\ttrain's binary_logloss: 0.360934\tval's binary_logloss: 0.447443\n",
      "[304]\ttrain's binary_logloss: 0.360591\tval's binary_logloss: 0.447438\n",
      "[305]\ttrain's binary_logloss: 0.360357\tval's binary_logloss: 0.447509\n",
      "[306]\ttrain's binary_logloss: 0.360095\tval's binary_logloss: 0.447447\n",
      "[307]\ttrain's binary_logloss: 0.359885\tval's binary_logloss: 0.447397\n",
      "[308]\ttrain's binary_logloss: 0.359664\tval's binary_logloss: 0.447402\n",
      "[309]\ttrain's binary_logloss: 0.359502\tval's binary_logloss: 0.447429\n",
      "[310]\ttrain's binary_logloss: 0.359299\tval's binary_logloss: 0.447384\n",
      "[311]\ttrain's binary_logloss: 0.359222\tval's binary_logloss: 0.447389\n",
      "[312]\ttrain's binary_logloss: 0.358902\tval's binary_logloss: 0.447382\n",
      "[313]\ttrain's binary_logloss: 0.358671\tval's binary_logloss: 0.447334\n",
      "[314]\ttrain's binary_logloss: 0.358434\tval's binary_logloss: 0.44734\n",
      "[315]\ttrain's binary_logloss: 0.358113\tval's binary_logloss: 0.447249\n",
      "[316]\ttrain's binary_logloss: 0.357775\tval's binary_logloss: 0.447257\n",
      "[317]\ttrain's binary_logloss: 0.357478\tval's binary_logloss: 0.447226\n",
      "[318]\ttrain's binary_logloss: 0.357333\tval's binary_logloss: 0.447235\n",
      "[319]\ttrain's binary_logloss: 0.357118\tval's binary_logloss: 0.447302\n",
      "[320]\ttrain's binary_logloss: 0.356932\tval's binary_logloss: 0.447322\n",
      "[321]\ttrain's binary_logloss: 0.356813\tval's binary_logloss: 0.447336\n",
      "[322]\ttrain's binary_logloss: 0.356501\tval's binary_logloss: 0.447373\n",
      "[323]\ttrain's binary_logloss: 0.356381\tval's binary_logloss: 0.447424\n",
      "[324]\ttrain's binary_logloss: 0.356079\tval's binary_logloss: 0.447365\n",
      "[325]\ttrain's binary_logloss: 0.355868\tval's binary_logloss: 0.447445\n",
      "[326]\ttrain's binary_logloss: 0.355622\tval's binary_logloss: 0.447553\n",
      "[327]\ttrain's binary_logloss: 0.355395\tval's binary_logloss: 0.44754\n",
      "Early stopping, best iteration is:\n",
      "[227]\ttrain's binary_logloss: 0.379541\tval's binary_logloss: 0.446921\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LGBMClassifier(colsample_bytree=0.7, max_depth=6, min_child_weight=0.5,\n",
       "               n_estimators=1000, num_leaves=50, objective='binary',\n",
       "               subsample=0.8)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbm.fit(x_train, y_train, \n",
    "        eval_set=[(x_train, y_train), (x_val, y_val)], \n",
    "        eval_names=['train', 'val'],\n",
    "        eval_metric='binary_logloss',\n",
    "        early_stopping_rounds=100,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.特征转换并构建新的数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#获取构建的树\n",
    "model = gbm.booster_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 39)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_gbdt = pd.concat([x_train, x_val], axis=0)\n",
    "data_gbdt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#每个样本落在每个树的位置，下面的矩阵（样本个数，树的颗数），每一个数字代表某个样本落在了某个树的哪个叶子节点上\n",
    "gbdt_feats = model.predict(data_gbdt, pred_leaf=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbdt_feats_name = ['gbdt_leaf_' + str(i) for i in range(gbdt_feats.shape[1])]\n",
    "df_gbdt_feats = pd.DataFrame(gbdt_feats, columns = gbdt_feats_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 227)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_gbdt_feats.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 266)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_gbdt = pd.concat([data_gbdt, df_gbdt_feats], axis = 1)\n",
    "\n",
    "data_gbdt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>I1</th>\n",
       "      <th>I2</th>\n",
       "      <th>I3</th>\n",
       "      <th>I4</th>\n",
       "      <th>I5</th>\n",
       "      <th>I6</th>\n",
       "      <th>I7</th>\n",
       "      <th>I8</th>\n",
       "      <th>I9</th>\n",
       "      <th>I10</th>\n",
       "      <th>...</th>\n",
       "      <th>gbdt_leaf_217</th>\n",
       "      <th>gbdt_leaf_218</th>\n",
       "      <th>gbdt_leaf_219</th>\n",
       "      <th>gbdt_leaf_220</th>\n",
       "      <th>gbdt_leaf_221</th>\n",
       "      <th>gbdt_leaf_222</th>\n",
       "      <th>gbdt_leaf_223</th>\n",
       "      <th>gbdt_leaf_224</th>\n",
       "      <th>gbdt_leaf_225</th>\n",
       "      <th>gbdt_leaf_226</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>1.791759</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.232010</td>\n",
       "      <td>1.609438</td>\n",
       "      <td>2.772589</td>\n",
       "      <td>1.098612</td>\n",
       "      <td>5.204007</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>...</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>39</td>\n",
       "      <td>30</td>\n",
       "      <td>12</td>\n",
       "      <td>10</td>\n",
       "      <td>41</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.098612</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.806662</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>4.634729</td>\n",
       "      <td>2.197225</td>\n",
       "      <td>1.098612</td>\n",
       "      <td>1.098612</td>\n",
       "      <td>1.609438</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>...</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "      <td>7</td>\n",
       "      <td>28</td>\n",
       "      <td>31</td>\n",
       "      <td>12</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.098612</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>2.708050</td>\n",
       "      <td>6.643790</td>\n",
       "      <td>4.499810</td>\n",
       "      <td>1.609438</td>\n",
       "      <td>1.098612</td>\n",
       "      <td>5.505332</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>...</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>31</td>\n",
       "      <td>31</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.795706</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.387768</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "      <td>7</td>\n",
       "      <td>28</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.386294</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.098612</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>...</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "      <td>31</td>\n",
       "      <td>38</td>\n",
       "      <td>43</td>\n",
       "      <td>20</td>\n",
       "      <td>28</td>\n",
       "      <td>43</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.459152</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.945910</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>36</td>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>41</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>1.098612</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.061171</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>1.098612</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "      <td>4</td>\n",
       "      <td>28</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>13</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.693147</td>\n",
       "      <td>1.609438</td>\n",
       "      <td>1.098612</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>...</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>36</td>\n",
       "      <td>41</td>\n",
       "      <td>12</td>\n",
       "      <td>28</td>\n",
       "      <td>41</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.806662</td>\n",
       "      <td>1.609438</td>\n",
       "      <td>2.197225</td>\n",
       "      <td>9.852773</td>\n",
       "      <td>5.521461</td>\n",
       "      <td>3.367296</td>\n",
       "      <td>3.465736</td>\n",
       "      <td>4.955827</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "      <td>33</td>\n",
       "      <td>28</td>\n",
       "      <td>31</td>\n",
       "      <td>12</td>\n",
       "      <td>10</td>\n",
       "      <td>24</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.583519</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>10.426380</td>\n",
       "      <td>3.091042</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>1.098612</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 266 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         I1        I2        I3        I4         I5        I6        I7  \\\n",
       "0  0.693147  0.693147  1.791759  0.000000   7.232010  1.609438  2.772589   \n",
       "1  1.098612  0.000000  3.806662  0.693147   4.634729  2.197225  1.098612   \n",
       "2  1.098612  0.000000  0.693147  2.708050   6.643790  4.499810  1.609438   \n",
       "3  0.000000  6.795706  0.000000  0.000000   8.387768  0.000000  0.000000   \n",
       "4  1.386294 -1.000000  0.000000  0.000000   1.098612  0.000000  1.386294   \n",
       "5  0.000000 -1.000000  0.000000  0.000000   9.459152  0.000000  0.000000   \n",
       "6  0.000000  0.693147  1.098612  0.000000   8.061171  0.000000  0.000000   \n",
       "7  0.693147  1.609438  1.098612  0.000000   0.000000  0.000000  0.693147   \n",
       "8  0.000000  3.806662  1.609438  2.197225   9.852773  5.521461  3.367296   \n",
       "9  0.000000  3.583519  0.000000  0.693147  10.426380  3.091042  0.693147   \n",
       "\n",
       "         I8        I9       I10  ...  gbdt_leaf_217  gbdt_leaf_218  \\\n",
       "0  1.098612  5.204007  0.693147  ...             24              0   \n",
       "1  1.098612  1.609438  0.693147  ...             24              0   \n",
       "2  1.098612  5.505332  0.693147  ...             24              0   \n",
       "3  0.000000  0.000000  0.000000  ...             23              0   \n",
       "4  0.000000  0.000000  0.693147  ...             24              0   \n",
       "5  0.000000  1.945910  0.000000  ...             24              0   \n",
       "6  0.693147  1.098612  0.000000  ...             24              0   \n",
       "7  0.000000  0.000000  0.693147  ...             24              0   \n",
       "8  3.465736  4.955827  0.000000  ...             24              0   \n",
       "9  1.098612  1.386294  0.000000  ...             24              0   \n",
       "\n",
       "   gbdt_leaf_219  gbdt_leaf_220  gbdt_leaf_221  gbdt_leaf_222  gbdt_leaf_223  \\\n",
       "0             29              0             39             30             12   \n",
       "1             29              7             28             31             12   \n",
       "2             29              0             31             31             12   \n",
       "3             27              7             28             11             12   \n",
       "4             29             31             38             43             20   \n",
       "5             29              0             36             30              1   \n",
       "6             29              4             28             31              0   \n",
       "7             11              0             36             41             12   \n",
       "8             29             33             28             31             12   \n",
       "9             29              0             28             37              1   \n",
       "\n",
       "   gbdt_leaf_224  gbdt_leaf_225  gbdt_leaf_226  \n",
       "0             10             41             44  \n",
       "1             10              9             44  \n",
       "2              0             13             44  \n",
       "3              0             13             12  \n",
       "4             28             43             43  \n",
       "5             25             41             42  \n",
       "6             10             13             44  \n",
       "7             28             41             20  \n",
       "8             10             24             44  \n",
       "9              0              0             44  \n",
       "\n",
       "[10 rows x 266 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_gbdt.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.将GBDT子树的特征向量做one hot编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 新数据的新特征进行one hot编码\n",
    "for col in gbdt_feats_name:\n",
    "    onehot_feats = pd.get_dummies(data_gbdt[col], prefix=col)\n",
    "    data_gbdt.drop([col], axis=1, inplace=True)\n",
    "    data_gbdt = pd.concat([data_gbdt, onehot_feats], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 连续特征做归一化处理\n",
    "scaler = MinMaxScaler()\n",
    "for col in dense_feats:\n",
    "    data_gbdt[col] = scaler.fit_transform(data_gbdt[col].values.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 划分数据集\n",
    "target = data_df['label']\n",
    "x_train, x_val, y_train, y_val = train_test_split(data_gbdt, target, test_size=0.2,random_state=2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80000, 9821)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.训练逻辑回归模型作最后的预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tr_logloss: 0.5349669457086674\n",
      "val_logloss: 0.541720575861628\n"
     ]
    }
   ],
   "source": [
    "#训练逻辑回归模型\n",
    "lr = LogisticRegression()\n",
    "lr.fit(x_train, y_train)\n",
    "tr_logloss = log_loss(y_train, lr.predict_proba(x_train)[:,1])\n",
    "print('tr_logloss:', tr_logloss)\n",
    "val_logloss = log_loss(y_val, lr.predict_proba(x_val)[:,1])\n",
    "print('val_logloss:', val_logloss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf2]",
   "language": "python",
   "name": "conda-env-tf2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
